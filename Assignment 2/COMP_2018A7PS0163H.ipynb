{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read and shuffle data\n",
    "df = pd.read_csv(\"dataset_comb.csv\")\n",
    "df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jasmine    9985\n",
       "Gonen      8200\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Roundness</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>5463</td>\n",
       "      <td>130.964374</td>\n",
       "      <td>54.286664</td>\n",
       "      <td>0.910043</td>\n",
       "      <td>5629</td>\n",
       "      <td>83.400885</td>\n",
       "      <td>0.598685</td>\n",
       "      <td>306.634</td>\n",
       "      <td>0.730130</td>\n",
       "      <td>2.412459</td>\n",
       "      <td>jasmine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15046</th>\n",
       "      <td>9205</td>\n",
       "      <td>162.685029</td>\n",
       "      <td>72.800534</td>\n",
       "      <td>0.894287</td>\n",
       "      <td>9495</td>\n",
       "      <td>108.259734</td>\n",
       "      <td>0.658770</td>\n",
       "      <td>390.435</td>\n",
       "      <td>0.758815</td>\n",
       "      <td>2.234668</td>\n",
       "      <td>Gonen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14541</th>\n",
       "      <td>8837</td>\n",
       "      <td>158.905855</td>\n",
       "      <td>71.670264</td>\n",
       "      <td>0.892512</td>\n",
       "      <td>9070</td>\n",
       "      <td>106.073644</td>\n",
       "      <td>0.766170</td>\n",
       "      <td>377.847</td>\n",
       "      <td>0.777827</td>\n",
       "      <td>2.217180</td>\n",
       "      <td>Gonen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7242</th>\n",
       "      <td>6062</td>\n",
       "      <td>154.817347</td>\n",
       "      <td>50.606843</td>\n",
       "      <td>0.945066</td>\n",
       "      <td>6210</td>\n",
       "      <td>87.854301</td>\n",
       "      <td>0.729922</td>\n",
       "      <td>341.458</td>\n",
       "      <td>0.653358</td>\n",
       "      <td>3.059218</td>\n",
       "      <td>jasmine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11488</th>\n",
       "      <td>7301</td>\n",
       "      <td>141.611305</td>\n",
       "      <td>67.271861</td>\n",
       "      <td>0.879961</td>\n",
       "      <td>7546</td>\n",
       "      <td>96.415361</td>\n",
       "      <td>0.791008</td>\n",
       "      <td>343.633</td>\n",
       "      <td>0.776967</td>\n",
       "      <td>2.105060</td>\n",
       "      <td>Gonen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14764</th>\n",
       "      <td>8291</td>\n",
       "      <td>154.207560</td>\n",
       "      <td>69.296615</td>\n",
       "      <td>0.893345</td>\n",
       "      <td>8464</td>\n",
       "      <td>102.744484</td>\n",
       "      <td>0.567255</td>\n",
       "      <td>365.457</td>\n",
       "      <td>0.780089</td>\n",
       "      <td>2.225326</td>\n",
       "      <td>Gonen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>6815</td>\n",
       "      <td>130.582768</td>\n",
       "      <td>67.568802</td>\n",
       "      <td>0.855719</td>\n",
       "      <td>7060</td>\n",
       "      <td>93.151100</td>\n",
       "      <td>0.701926</td>\n",
       "      <td>325.713</td>\n",
       "      <td>0.807245</td>\n",
       "      <td>1.932590</td>\n",
       "      <td>Gonen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5726</th>\n",
       "      <td>5664</td>\n",
       "      <td>147.931670</td>\n",
       "      <td>49.798089</td>\n",
       "      <td>0.941637</td>\n",
       "      <td>5806</td>\n",
       "      <td>84.921309</td>\n",
       "      <td>0.558580</td>\n",
       "      <td>326.076</td>\n",
       "      <td>0.669415</td>\n",
       "      <td>2.970629</td>\n",
       "      <td>jasmine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>6281</td>\n",
       "      <td>144.640305</td>\n",
       "      <td>56.509156</td>\n",
       "      <td>0.920523</td>\n",
       "      <td>6402</td>\n",
       "      <td>89.427164</td>\n",
       "      <td>0.708437</td>\n",
       "      <td>336.952</td>\n",
       "      <td>0.695189</td>\n",
       "      <td>2.559591</td>\n",
       "      <td>jasmine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8397</th>\n",
       "      <td>7697</td>\n",
       "      <td>141.004638</td>\n",
       "      <td>70.076110</td>\n",
       "      <td>0.867764</td>\n",
       "      <td>7952</td>\n",
       "      <td>98.995580</td>\n",
       "      <td>0.619577</td>\n",
       "      <td>374.104</td>\n",
       "      <td>0.691109</td>\n",
       "      <td>2.012164</td>\n",
       "      <td>Gonen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18185 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Area  MajorAxisLength  MinorAxisLength  Eccentricity  ConvexArea  \\\n",
       "421    5463       130.964374        54.286664      0.910043        5629   \n",
       "15046  9205       162.685029        72.800534      0.894287        9495   \n",
       "14541  8837       158.905855        71.670264      0.892512        9070   \n",
       "7242   6062       154.817347        50.606843      0.945066        6210   \n",
       "11488  7301       141.611305        67.271861      0.879961        7546   \n",
       "...     ...              ...              ...           ...         ...   \n",
       "14764  8291       154.207560        69.296615      0.893345        8464   \n",
       "6551   6815       130.582768        67.568802      0.855719        7060   \n",
       "5726   5664       147.931670        49.798089      0.941637        5806   \n",
       "755    6281       144.640305        56.509156      0.920523        6402   \n",
       "8397   7697       141.004638        70.076110      0.867764        7952   \n",
       "\n",
       "       EquivDiameter    Extent  Perimeter  Roundness  AspectRation    Class  \n",
       "421        83.400885  0.598685    306.634   0.730130      2.412459  jasmine  \n",
       "15046     108.259734  0.658770    390.435   0.758815      2.234668    Gonen  \n",
       "14541     106.073644  0.766170    377.847   0.777827      2.217180    Gonen  \n",
       "7242       87.854301  0.729922    341.458   0.653358      3.059218  jasmine  \n",
       "11488      96.415361  0.791008    343.633   0.776967      2.105060    Gonen  \n",
       "...              ...       ...        ...        ...           ...      ...  \n",
       "14764     102.744484  0.567255    365.457   0.780089      2.225326    Gonen  \n",
       "6551       93.151100  0.701926    325.713   0.807245      1.932590    Gonen  \n",
       "5726       84.921309  0.558580    326.076   0.669415      2.970629  jasmine  \n",
       "755        89.427164  0.708437    336.952   0.695189      2.559591  jasmine  \n",
       "8397       98.995580  0.619577    374.104   0.691109      2.012164    Gonen  \n",
       "\n",
       "[18185 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['id'], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min-max normalisation\n",
    "#convert class variables:   0 - jasmine & 1 - Gonen\n",
    "for column in df.columns:\n",
    "    if column!='Class':\n",
    "        maxx = df[column].max()\n",
    "        minn = df[column].min()\n",
    "        for e in df[column]:\n",
    "            e1 = (maxx - e)/(maxx - minn)\n",
    "            df[column] = df[column].replace(e, e1)\n",
    "    elif column == 'Class':\n",
    "        for e in df[column]:\n",
    "            if e == 'jasmine':\n",
    "                df[column] = df[column].replace(e, 0)\n",
    "            elif e == 'Gonen':\n",
    "                df[column] = df[column].replace(e, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Roundness</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.617456</td>\n",
       "      <td>0.478987</td>\n",
       "      <td>0.587112</td>\n",
       "      <td>0.195539</td>\n",
       "      <td>0.638154</td>\n",
       "      <td>1.954015</td>\n",
       "      <td>0.571963</td>\n",
       "      <td>0.648089</td>\n",
       "      <td>0.239151</td>\n",
       "      <td>0.587138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15046</th>\n",
       "      <td>0.130723</td>\n",
       "      <td>0.188180</td>\n",
       "      <td>0.202535</td>\n",
       "      <td>0.249845</td>\n",
       "      <td>0.179499</td>\n",
       "      <td>1.986336</td>\n",
       "      <td>0.452587</td>\n",
       "      <td>0.379061</td>\n",
       "      <td>0.199865</td>\n",
       "      <td>0.656759</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14541</th>\n",
       "      <td>0.178590</td>\n",
       "      <td>0.222827</td>\n",
       "      <td>0.226014</td>\n",
       "      <td>0.255963</td>\n",
       "      <td>0.229921</td>\n",
       "      <td>1.954014</td>\n",
       "      <td>0.239212</td>\n",
       "      <td>0.419472</td>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.663607</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7242</th>\n",
       "      <td>0.539542</td>\n",
       "      <td>0.260309</td>\n",
       "      <td>0.663551</td>\n",
       "      <td>0.074823</td>\n",
       "      <td>0.569225</td>\n",
       "      <td>1.954014</td>\n",
       "      <td>0.311227</td>\n",
       "      <td>1.627245</td>\n",
       "      <td>0.344296</td>\n",
       "      <td>0.333877</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11488</th>\n",
       "      <td>0.378382</td>\n",
       "      <td>0.381379</td>\n",
       "      <td>0.317379</td>\n",
       "      <td>0.299223</td>\n",
       "      <td>0.410725</td>\n",
       "      <td>1.953514</td>\n",
       "      <td>0.189865</td>\n",
       "      <td>0.529310</td>\n",
       "      <td>0.175005</td>\n",
       "      <td>0.707512</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14764</th>\n",
       "      <td>0.249610</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.275320</td>\n",
       "      <td>0.253093</td>\n",
       "      <td>0.301815</td>\n",
       "      <td>1.954014</td>\n",
       "      <td>0.634406</td>\n",
       "      <td>0.459248</td>\n",
       "      <td>0.170729</td>\n",
       "      <td>0.660417</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>0.441597</td>\n",
       "      <td>0.482485</td>\n",
       "      <td>0.311211</td>\n",
       "      <td>0.382779</td>\n",
       "      <td>0.468383</td>\n",
       "      <td>1.954014</td>\n",
       "      <td>0.366848</td>\n",
       "      <td>0.586839</td>\n",
       "      <td>0.133537</td>\n",
       "      <td>0.775049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5726</th>\n",
       "      <td>0.591311</td>\n",
       "      <td>0.323435</td>\n",
       "      <td>0.680351</td>\n",
       "      <td>0.086639</td>\n",
       "      <td>0.617155</td>\n",
       "      <td>1.954015</td>\n",
       "      <td>0.651641</td>\n",
       "      <td>1.627256</td>\n",
       "      <td>0.322305</td>\n",
       "      <td>0.368567</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>0.511056</td>\n",
       "      <td>0.353609</td>\n",
       "      <td>0.540946</td>\n",
       "      <td>0.159414</td>\n",
       "      <td>0.546447</td>\n",
       "      <td>1.954015</td>\n",
       "      <td>0.353913</td>\n",
       "      <td>1.630712</td>\n",
       "      <td>0.287006</td>\n",
       "      <td>0.529524</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8397</th>\n",
       "      <td>0.326873</td>\n",
       "      <td>0.386940</td>\n",
       "      <td>0.259128</td>\n",
       "      <td>0.341265</td>\n",
       "      <td>0.362558</td>\n",
       "      <td>1.983519</td>\n",
       "      <td>0.530456</td>\n",
       "      <td>0.431489</td>\n",
       "      <td>0.292593</td>\n",
       "      <td>0.743889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18185 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Area  MajorAxisLength  MinorAxisLength  Eccentricity  ConvexArea  \\\n",
       "421    0.617456         0.478987         0.587112      0.195539    0.638154   \n",
       "15046  0.130723         0.188180         0.202535      0.249845    0.179499   \n",
       "14541  0.178590         0.222827         0.226014      0.255963    0.229921   \n",
       "7242   0.539542         0.260309         0.663551      0.074823    0.569225   \n",
       "11488  0.378382         0.381379         0.317379      0.299223    0.410725   \n",
       "...         ...              ...              ...           ...         ...   \n",
       "14764  0.249610         0.265900         0.275320      0.253093    0.301815   \n",
       "6551   0.441597         0.482485         0.311211      0.382779    0.468383   \n",
       "5726   0.591311         0.323435         0.680351      0.086639    0.617155   \n",
       "755    0.511056         0.353609         0.540946      0.159414    0.546447   \n",
       "8397   0.326873         0.386940         0.259128      0.341265    0.362558   \n",
       "\n",
       "       EquivDiameter    Extent  Perimeter  Roundness  AspectRation  Class  \n",
       "421         1.954015  0.571963   0.648089   0.239151      0.587138      0  \n",
       "15046       1.986336  0.452587   0.379061   0.199865      0.656759      1  \n",
       "14541       1.954014  0.239212   0.419472   0.173828      0.663607      1  \n",
       "7242        1.954014  0.311227   1.627245   0.344296      0.333877      0  \n",
       "11488       1.953514  0.189865   0.529310   0.175005      0.707512      1  \n",
       "...              ...       ...        ...        ...           ...    ...  \n",
       "14764       1.954014  0.634406   0.459248   0.170729      0.660417      1  \n",
       "6551        1.954014  0.366848   0.586839   0.133537      0.775049      1  \n",
       "5726        1.954015  0.651641   1.627256   0.322305      0.368567      0  \n",
       "755         1.954015  0.353913   1.630712   0.287006      0.529524      0  \n",
       "8397        1.983519  0.530456   0.431489   0.292593      0.743889      1  \n",
       "\n",
       "[18185 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18185, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.to_numpy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into 7 folds\n",
    "data_g = np.array_split(data, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_log_test = []\n",
    "accuracy_log_train = []\n",
    "\n",
    "for i in range(7):\n",
    "    test_data = data_g[i]\n",
    "    test_data_x = []\n",
    "    test_data_y = []\n",
    "    train_data_x = []\n",
    "    train_data_y = []\n",
    "    for j in range(7):\n",
    "        if j!=i:\n",
    "            for w in range(len(data_g[j])):\n",
    "                train_data_x.append(data_g[j][w][:10])\n",
    "                train_data_y.append(data_g[j][w][10])\n",
    "        elif j == i:\n",
    "            for w in range(len(data_g[j])):\n",
    "                test_data_x.append(data_g[j][w][:10])\n",
    "                test_data_y.append(data_g[j][w][10])\n",
    "                \n",
    "    #convert into array\n",
    "    test_data_xn = np.array(test_data_x)\n",
    "    train_data_xn = np.array(train_data_x)\n",
    "    test_data_yn = np.array(test_data_y)\n",
    "    train_data_yn = np.array(train_data_y)\n",
    "    \n",
    "    #importing model from sklearn \n",
    "    clf = LogisticRegression(random_state = 0)\n",
    "    clf.fit(train_data_xn, train_data_yn)\n",
    "    \n",
    "    #predict class\n",
    "    test_pred_y_log = clf.predict(test_data_xn)\n",
    "    train_pred_y_log = clf.predict(train_data_xn)\n",
    "    \n",
    "    #calculating and appending accuracies\n",
    "    acc1_log = accuracy_score(test_data_yn, test_pred_y_log)\n",
    "    accuracy_log_test.append(acc1_log)\n",
    "    \n",
    "    acc2_log = accuracy_score(train_data_yn, train_pred_y_log)\n",
    "    accuracy_log_train.append(acc2_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for test set =  0.9870222436440697\n",
      "Mean accuracy for train set =  0.9870772606327832\n"
     ]
    }
   ],
   "source": [
    "#printing accuracies \n",
    "print(\"Mean accuracy for test set = \", statistics.mean(accuracy_log_test))\n",
    "print(\"Mean accuracy for train set = \", statistics.mean(accuracy_log_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_lp_test = []\n",
    "accuracy_lp_train = []\n",
    "\n",
    "for i in range(7):\n",
    "    test_data = data_g[i]\n",
    "    test_data_x = []\n",
    "    test_data_y = []\n",
    "    train_data_x = []\n",
    "    train_data_y = []\n",
    "    for j in range(7):\n",
    "        if j!=i:\n",
    "            for w in range(len(data_g[j])):\n",
    "                train_data_x.append(data_g[j][w][:10])\n",
    "                train_data_y.append(data_g[j][w][10])\n",
    "        elif j == i:\n",
    "            for w in range(len(data_g[j])):\n",
    "                test_data_x.append(data_g[j][w][:10])\n",
    "                test_data_y.append(data_g[j][w][10])\n",
    "\n",
    "    #convert into array  \n",
    "    test_data_xn = np.array(test_data_x)\n",
    "    train_data_xn = np.array(train_data_x)\n",
    "    test_data_yn = np.array(test_data_y)\n",
    "    train_data_yn = np.array(train_data_y)\n",
    "    \n",
    "    #importing model from sklearn\n",
    "    clf = Perceptron(tol = 1e-3, random_state=0)\n",
    "    clf.fit(train_data_xn, train_data_yn)\n",
    "    \n",
    "    #predict class\n",
    "    test_pred_y_lp = clf.predict(test_data_xn)\n",
    "    train_pred_y_lp = clf.predict(train_data_xn)\n",
    "    \n",
    "    #calculating and appending accuracies\n",
    "    acc1_lp = accuracy_score(test_data_yn, test_pred_y_lp)\n",
    "    accuracy_lp_test.append(acc1_lp)\n",
    "    \n",
    "    acc2_lp = accuracy_score(train_data_yn, train_pred_y_lp)\n",
    "    accuracy_lp_train.append(acc2_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for test set =  0.9797639342335167\n",
      "Mean accuracy for train set =  0.9787094769191089\n"
     ]
    }
   ],
   "source": [
    "#printing accuracies \n",
    "\n",
    "print(\"Mean accuracy for test set = \", statistics.mean(accuracy_lp_test))\n",
    "print(\"Mean accuracy for train set = \", statistics.mean(accuracy_lp_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_svm_test = []\n",
    "accuracy_svm_train = []\n",
    "\n",
    "for i in range(7):\n",
    "    test_data = data_g[i]\n",
    "    test_data_x = []\n",
    "    test_data_y = []\n",
    "    train_data_x = []\n",
    "    train_data_y = []\n",
    "    for j in range(7):\n",
    "        if j!=i:\n",
    "            for w in range(len(data_g[j])):\n",
    "                train_data_x.append(data_g[j][w][:10])\n",
    "                train_data_y.append(data_g[j][w][10])\n",
    "        elif j == i:\n",
    "            for w in range(len(data_g[j])):\n",
    "                test_data_x.append(data_g[j][w][:10])\n",
    "                test_data_y.append(data_g[j][w][10])\n",
    "                \n",
    "    #convert into array\n",
    "    test_data_xn = np.array(test_data_x)\n",
    "    train_data_xn = np.array(train_data_x)\n",
    "    test_data_yn = np.array(test_data_y)\n",
    "    train_data_yn = np.array(train_data_y)\n",
    "    \n",
    "    #importing model from sklearn\n",
    "    clf = SVC(kernel = 'rbf')\n",
    "    clf.fit(train_data_xn, train_data_yn)\n",
    "    \n",
    "    #predict class\n",
    "    test_pred_y_svm = clf.predict(test_data_xn)\n",
    "    train_pred_y_svm = clf.predict(train_data_xn)\n",
    "\n",
    "    #calculating and appending accuracies\n",
    "    acc1_svm = accuracy_score(test_data_yn, test_pred_y_svm)\n",
    "    accuracy_svm_test.append(acc1_svm)\n",
    "    \n",
    "    acc2_svm = accuracy_score(train_data_yn, train_pred_y_svm)\n",
    "    accuracy_svm_train.append(acc2_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for test set =  0.9887270633183709\n",
      "Mean accuracy for train set =  0.9886261598552634\n"
     ]
    }
   ],
   "source": [
    "#printing accuracies \n",
    "print(\"Mean accuracy for test set = \", statistics.mean(accuracy_svm_test))\n",
    "print(\"Mean accuracy for train set = \", statistics.mean(accuracy_svm_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_nb_test = []\n",
    "accuracy_nb_train = []\n",
    "\n",
    "for i in range(7):\n",
    "    test_data = data_g[i]\n",
    "    test_data_x = []\n",
    "    test_data_y = []\n",
    "    train_data_x = []\n",
    "    train_data_y = []\n",
    "    for j in range(7):\n",
    "        if j!=i:\n",
    "            for w in range(len(data_g[j])):\n",
    "                train_data_x.append(data_g[j][w][:10])\n",
    "                train_data_y.append(data_g[j][w][10])\n",
    "        elif j == i:\n",
    "            for w in range(len(data_g[j])):\n",
    "                test_data_x.append(data_g[j][w][:10])\n",
    "                test_data_y.append(data_g[j][w][10])\n",
    "       \n",
    "    #convert into array         \n",
    "    test_data_xn = np.array(test_data_x)\n",
    "    train_data_xn = np.array(train_data_x)\n",
    "    test_data_yn = np.array(test_data_y)\n",
    "    train_data_yn = np.array(train_data_y)\n",
    "    \n",
    "    #importing model from sklearn\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(train_data_xn, train_data_yn)\n",
    "    \n",
    "    #predict class\n",
    "    test_pred_y_nb = clf.predict(test_data_xn)\n",
    "    train_pred_y_nb = clf.predict(train_data_xn)\n",
    "\n",
    "    #calculating and appending accuracies\n",
    "    acc1_nb = accuracy_score(test_data_yn, test_pred_y_nb)\n",
    "    accuracy_nb_test.append(acc1_nb)\n",
    "    \n",
    "    acc2_nb = accuracy_score(train_data_yn, train_pred_y_nb)\n",
    "    accuracy_nb_train.append(acc2_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for test set =  0.9846025883819537\n",
      "Mean accuracy for train set =  0.9846210248142574\n"
     ]
    }
   ],
   "source": [
    "#printing accuracies \n",
    "print(\"Mean accuracy for test set = \", statistics.mean(accuracy_nb_test))\n",
    "print(\"Mean accuracy for train set = \", statistics.mean(accuracy_nb_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Linear Discriminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_fl_test = []\n",
    "accuracy_fl_train = []\n",
    "\n",
    "for i in range(7):\n",
    "    test_data = data_g[i]\n",
    "    test_data_x = []\n",
    "    test_data_y = []\n",
    "    train_data_x = []\n",
    "    train_data_y = []\n",
    "    for j in range(7):\n",
    "        if j!=i:\n",
    "            for w in range(len(data_g[j])):\n",
    "                train_data_x.append(data_g[j][w][:10])\n",
    "                train_data_y.append(data_g[j][w][10])\n",
    "        elif j == i:\n",
    "            for w in range(len(data_g[j])):\n",
    "                test_data_x.append(data_g[j][w][:10])\n",
    "                test_data_y.append(data_g[j][w][10])\n",
    "                \n",
    "    #convert into array\n",
    "    test_data_xn = np.array(test_data_x)\n",
    "    train_data_xn = np.array(train_data_x)\n",
    "    test_data_yn = np.array(test_data_y)\n",
    "    train_data_yn = np.array(train_data_y)\n",
    "    \n",
    "    #importing model from sklearn\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(train_data_xn, train_data_yn)\n",
    "    \n",
    "    #predict class\n",
    "    test_pred_y_fl = clf.predict(test_data_xn)\n",
    "    train_pred_y_fl = clf.predict(train_data_xn)\n",
    "    \n",
    "    #calculating and appending accuracies\n",
    "    acc1_fl = accuracy_score(test_data_yn, test_pred_y_fl)\n",
    "    accuracy_fl_test.append(acc1_fl)\n",
    "    \n",
    "    acc2_fl = accuracy_score(train_data_yn, train_pred_y_fl)\n",
    "    accuracy_fl_train.append(acc2_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for test set =  0.9840527995465163\n",
      "Mean accuracy for train set =  0.9840986155635392\n"
     ]
    }
   ],
   "source": [
    "#printing accuracies\n",
    "print(\"Mean accuracy for test set = \", statistics.mean(accuracy_fl_test))\n",
    "print(\"Mean accuracy for train set = \", statistics.mean(accuracy_fl_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ann_test = []\n",
    "accuracy_ann_train = []\n",
    "\n",
    "for i in range(7):\n",
    "    test_data = data_g[i]\n",
    "    test_data_x = []\n",
    "    test_data_y = []\n",
    "    train_data_x = []\n",
    "    train_data_y = []\n",
    "    for j in range(7):\n",
    "        if j!=i:\n",
    "            for w in range(len(data_g[j])):\n",
    "                train_data_x.append(data_g[j][w][:10])\n",
    "                train_data_y.append(data_g[j][w][10])\n",
    "        elif j == i:\n",
    "            for w in range(len(data_g[j])):\n",
    "                test_data_x.append(data_g[j][w][:10])\n",
    "                test_data_y.append(data_g[j][w][10])\n",
    "                \n",
    "    #convert into array\n",
    "    test_data_xn = np.array(test_data_x)\n",
    "    train_data_xn = np.array(train_data_x)\n",
    "    test_data_yn = np.array(test_data_y)\n",
    "    train_data_yn = np.array(train_data_y)\n",
    "    \n",
    "    #importing model from sklearn\n",
    "    clf = MLPClassifier(hidden_layer_sizes = (10, 10, 10, 10,), max_iter = 1000, activation = 'logistic', solver = 'adam', random_state = 1)\n",
    "    \n",
    "    clf.fit(train_data_xn, train_data_yn)\n",
    "    \n",
    "    #predict class\n",
    "    test_pred_y_ann = clf.predict(test_data_xn)\n",
    "    train_pred_y_ann = clf.predict(train_data_xn)\n",
    "    \n",
    "    #calculating and appending accuracies\n",
    "    acc1_ann = accuracy_score(test_data_yn, test_pred_y_ann)\n",
    "    accuracy_ann_test.append(acc1_ann)\n",
    "    \n",
    "    acc2_ann = accuracy_score(train_data_yn, train_pred_y_ann)\n",
    "    accuracy_ann_train.append(acc2_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for test set =  0.9875721806933963\n",
      "Mean accuracy for train set =  0.9878746189952411\n"
     ]
    }
   ],
   "source": [
    "#printing accuracies \n",
    "print(\"Mean accuracy for test set = \", statistics.mean(accuracy_ann_test))\n",
    "print(\"Mean accuracy for train set = \", statistics.mean(accuracy_ann_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (20, 10))\n",
    "plt.boxplot([accuracy_log_test, accuracy_lp_test, accuracy_svm_test, accuracy_nb_test, accuracy_fl_test, accuracy_ann_test])\n",
    "\n",
    "plt.text(0.75, 0.98, \"Logistic Regression\")\n",
    "plt.text(0.80, 0.975, \"Mean: \" + str(round(statistics.mean(accuracy_log_test), 5)))\n",
    "\n",
    "plt.text(1.75, 0.99, \"Linear Perceptron\")\n",
    "plt.text(1.82, 0.975, \"Mean: \" + str(round(statistics.mean(accuracy_lp_test), 5)))\n",
    "\n",
    "plt.text(2.70, 0.98, \"Support Vector Machine\")\n",
    "plt.text(2.82, 0.975, \"Mean: \" + str(round(statistics.mean(accuracy_svm_test), 5)))\n",
    "\n",
    "plt.text(3.85, 0.98, \"Naive Bayes\")\n",
    "plt.text(3.80, 0.975, \"Mean: \" + str(round(statistics.mean(accuracy_nb_test), 5)))\n",
    "\n",
    "plt.text(4.68, 0.98, \"Fisher Linear Discriminant\")\n",
    "plt.text(4.80, 0.975, \"Mean: \" + str(round(statistics.mean(accuracy_fl_test), 5)))\n",
    "\n",
    "plt.text(5.68, 0.98, \"Artificial Neural Networks\")\n",
    "plt.text(5.80, 0.975, \"Mean: \" + str(round(statistics.mean(accuracy_ann_test), 5)))\n",
    "\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
